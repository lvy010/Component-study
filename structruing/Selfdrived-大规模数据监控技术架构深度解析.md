# Selfdrived å¤§è§„æ¨¡æ•°æ®ç›‘æ§æŠ€æœ¯æ¶æ„

## ğŸ“– æ¦‚è¿°

åœ¨ç°ä»£æ•°å­—åŒ–æ—¶ä»£ï¼Œä¼ä¸šéœ€è¦å¤„ç†å’Œç›‘æ§æµ·é‡ä¿¡æ¯æµï¼Œä»ç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ°ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼Œä»ä¸šåŠ¡æŒ‡æ ‡åˆ°å®‰å…¨äº‹ä»¶ã€‚

Selfdrived æ¶æ„æä¾›äº†ä¸€å¥—å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°å¯¹å¤§é‡ä¿¡æ¯çš„æŒç»­ã€å®æ—¶ã€æ™ºèƒ½ç›‘æ§ã€‚æœ¬æ–‡å°†è§£æè¿™ä¸€æ¶æ„çš„æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶å’Œå®ç°æ–¹æ³•ã€‚

## ğŸ—ï¸ æ•´ä½“æ¶æ„æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A[å¤šèŠ‚ç‚¹é‡‡é›†å™¨] --> B[æ•°æ®æ¸…æ´—]
        B --> C[æ¶ˆæ¯é˜Ÿåˆ— Kafka]
    end
  
    subgraph "æµå¤„ç†å±‚"
        C --> D[Apache Flink/Spark Streaming]
        D --> E[çª—å£è®¡ç®—]
        E --> F[çŠ¶æ€ç®¡ç†]
    end
  
    subgraph "å­˜å‚¨å±‚"
        F --> G[çƒ­æ•°æ®å­˜å‚¨ Redis/SSD]
        F --> H[å†·æ•°æ®å­˜å‚¨ å¯¹è±¡å­˜å‚¨]
        F --> I[Elasticsearch ç´¢å¼•]
    end
  
    subgraph "åˆ†æå±‚"
        I --> J[ç»Ÿè®¡é˜ˆå€¼æ£€æµ‹]
        I --> K[æœºå™¨å­¦ä¹ æ¨¡å‹]
        I --> L[è§„åˆ™å¼•æ“]
    end
  
    subgraph "å±•ç¤ºå±‚"
        J --> M[å¯è§†åŒ–çœ‹æ¿]
        K --> M
        L --> M
    end
  
    subgraph "èµ„æºç®¡ç†"
        N[èµ„æºç®¡ç†å™¨] --> A
        N --> D
        N --> G
    end
```

## 1. åˆ†å¸ƒå¼æ•°æ®é‡‡é›†ä¸å¤„ç† ğŸ”„

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

åˆ†å¸ƒå¼æ¶æ„æ˜¯å¤„ç†æµ·é‡æ•°æ®çš„åŸºç¡€ï¼Œé€šè¿‡å°†æ•°æ®é‡‡é›†ä»»åŠ¡åˆ†æ•£åˆ°å¤šä¸ªèŠ‚ç‚¹ï¼Œé¿å…å•ç‚¹æ•…éšœå’Œæ€§èƒ½ç“¶é¢ˆã€‚

### æŠ€æœ¯å®ç°æ¶æ„

```python
# æ•°æ®é‡‡é›†èŠ‚ç‚¹é…ç½®ç¤ºä¾‹
class DataCollectorNode:
    def __init__(self, node_id, node_type="worker"):
        self.node_id = node_id
        self.node_type = node_type
        self.status = "active"
        self.capacity = 1000  # æ¯ç§’å¤„ç†èƒ½åŠ›
      
    def collect_data(self, data_source):
        """æ•°æ®é‡‡é›†ä¸»æ–¹æ³•"""
        raw_data = self.fetch_from_source(data_source)
        cleaned_data = self.initial_cleaning(raw_data)
        self.send_to_queue(cleaned_data)
      
    def initial_cleaning(self, raw_data):
        """åˆæ­¥æ•°æ®æ¸…æ´—"""
        # å»é™¤æ— æ•ˆå­—æ®µã€æ ¼å¼æ ‡å‡†åŒ–ã€æ•°æ®éªŒè¯
        return {
            "timestamp": raw_data.get("timestamp"),
            "source": raw_data.get("source"),
            "data": self.validate_and_normalize(raw_data.get("data")),
            "metadata": raw_data.get("metadata", {})
        }
```

### åŠ¨æ€æ‰©å±•æœºåˆ¶

```yaml
# Kubernetes éƒ¨ç½²é…ç½®ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-collector
spec:
  replicas: 3  # åˆå§‹èŠ‚ç‚¹æ•°
  selector:
    matchLabels:
      app: data-collector
  template:
    metadata:
      labels:
        app: data-collector
    spec:
      containers:
      - name: collector
        image: selfdrived/data-collector:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: collector-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: data-collector
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ

```java
// Kafka ç”Ÿäº§è€…é…ç½®
@Configuration
public class KafkaProducerConfig {
  
    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-cluster:9092");
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
      
        // é«˜ååé‡ä¼˜åŒ–é…ç½®
        configProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        configProps.put(ProducerConfig.LINGER_MS_CONFIG, 10);
        configProps.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
      
        return new DefaultKafkaProducerFactory<>(configProps);
    }
}
```

## 2. å®æ—¶æµå¤„ç†æŠ€æœ¯ âš¡

### Apache Flink æµå¤„ç†å®ç°

```java
// Flink æµå¤„ç†ä½œä¸š
public class RealTimeMonitoringJob {
  
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
      
        // é…ç½®æ£€æŸ¥ç‚¹
        env.enableCheckpointing(60000); // 1åˆ†é’Ÿæ£€æŸ¥ç‚¹
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
      
        // æ•°æ®æº
        DataStream<MonitoringEvent> source = env
            .addSource(new FlinkKafkaConsumer<>("monitoring-events", 
                new MonitoringEventDeserializer(), kafkaProps))
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.<MonitoringEvent>forBoundedOutOfOrderness(Duration.ofSeconds(20))
                    .withTimestampAssigner((event, timestamp) -> event.getEventTime()));
      
        // çª—å£å¤„ç†
        source
            .keyBy(MonitoringEvent::getSourceSystem)
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new MonitoringAggregator())
            .addSink(new AlertingSink());
      
        env.execute("Real-time Monitoring Job");
    }
}

// è‡ªå®šä¹‰èšåˆå™¨
public class MonitoringAggregator implements AggregateFunction<MonitoringEvent, EventAccumulator, AlertResult> {
  
    @Override
    public EventAccumulator add(MonitoringEvent event, EventAccumulator accumulator) {
        accumulator.addEvent(event);
        accumulator.updateMetrics(event);
        return accumulator;
    }
  
    @Override
    public AlertResult getResult(EventAccumulator accumulator) {
        return new AlertResult(
            accumulator.getWindowStart(),
            accumulator.getWindowEnd(),
            accumulator.getEventCount(),
            accumulator.getAnomalyScore(),
            accumulator.getSourceSystem()
        );
    }
}
```

### çª—å£è®¡ç®—ç­–ç•¥

```scala
// Spark Streaming çª—å£å¤„ç†ç¤ºä¾‹
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

object WindowedProcessing {
  
  def processWithSlidingWindow(stream: DStream[MonitoringEvent]): DStream[WindowedResult] = {
    stream
      .window(windowLength = Seconds(300), slideInterval = Seconds(60)) // 5åˆ†é’Ÿçª—å£ï¼Œ1åˆ†é’Ÿæ»‘åŠ¨
      .map(event => (event.sourceSystem, event))
      .groupByKey()
      .mapValues(events => {
        val eventList = events.toList
        WindowedResult(
          eventCount = eventList.size,
          avgLatency = calculateAvgLatency(eventList),
          errorRate = calculateErrorRate(eventList),
          anomalyScore = detectAnomalies(eventList)
        )
      })
  }
  
  def processWithTumblingWindow(stream: DStream[MonitoringEvent]): DStream[TumblingResult] = {
    stream
      .window(windowLength = Seconds(60), slideInterval = Seconds(60)) // 1åˆ†é’Ÿæ»šåŠ¨çª—å£
      .transform(rdd => {
        rdd.groupBy(_.sourceSystem)
           .mapValues(calculateMetrics)
      })
  }
}
```

## 3. åˆ†å±‚å­˜å‚¨ä¸ç´¢å¼•ä¼˜åŒ– ğŸ—„ï¸

### å†·çƒ­æ•°æ®åˆ†å±‚ç­–ç•¥

```python
class DataTieringManager:
    def __init__(self):
        self.hot_storage = RedisCluster(hosts=['redis1:6379', 'redis2:6379'])
        self.warm_storage = ClickHouseClient()
        self.cold_storage = S3Client()
      
    def store_data(self, data, priority="normal"):
        """æ ¹æ®æ•°æ®çƒ­åº¦å’Œä¼˜å…ˆçº§é€‰æ‹©å­˜å‚¨å±‚"""
        current_time = datetime.now()
      
        if self.is_hot_data(data, current_time):
            # çƒ­æ•°æ®å­˜å‚¨åˆ°Redis
            self.hot_storage.setex(
                name=f"hot:{data['id']}", 
                time=3600,  # 1å°æ—¶TTL
                value=json.dumps(data)
            )
          
        elif self.is_warm_data(data, current_time):
            # æ¸©æ•°æ®å­˜å‚¨åˆ°ClickHouse
            self.warm_storage.execute("""
                INSERT INTO monitoring_events 
                (timestamp, source_system, event_type, data) 
                VALUES
            """, (data['timestamp'], data['source'], data['type'], data['payload']))
          
        else:
            # å†·æ•°æ®å­˜å‚¨åˆ°S3
            s3_key = f"cold_data/{data['timestamp'].strftime('%Y/%m/%d')}/{data['id']}.json"
            self.cold_storage.put_object(
                Bucket='monitoring-archive',
                Key=s3_key,
                Body=json.dumps(data)
            )
  
    def is_hot_data(self, data, current_time):
        """åˆ¤æ–­æ˜¯å¦ä¸ºçƒ­æ•°æ®ï¼ˆæœ€è¿‘1å°æ—¶ä¸”é«˜é¢‘è®¿é—®ï¼‰"""
        data_time = datetime.fromisoformat(data['timestamp'])
        time_diff = current_time - data_time
        return time_diff.total_seconds() < 3600 and data.get('access_frequency', 0) > 10
```

### Elasticsearch ç´¢å¼•ä¼˜åŒ–

```json
{
  "settings": {
    "number_of_shards": 12,
    "number_of_replicas": 1,
    "index.refresh_interval": "30s",
    "index.translog.flush_threshold_size": "1gb",
    "index.codec": "best_compression"
  },
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "source_system": {
        "type": "keyword",
        "index": true
      },
      "event_type": {
        "type": "keyword",
        "index": true
      },
      "severity": {
        "type": "keyword",
        "index": true
      },
      "message": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 256
          }
        }
      },
      "metrics": {
        "type": "nested",
        "properties": {
          "name": {"type": "keyword"},
          "value": {"type": "double"},
          "unit": {"type": "keyword"}
        }
      },
      "tags": {
        "type": "keyword",
        "index": true
      }
    }
  }
}
```

### å¤šçº§ç´¢å¼•ç­–ç•¥

```python
class MultiLevelIndexManager:
    def __init__(self):
        self.es_client = Elasticsearch(['es1:9200', 'es2:9200', 'es3:9200'])
      
    def create_time_based_indices(self):
        """åˆ›å»ºåŸºäºæ—¶é—´çš„ç´¢å¼•"""
        today = datetime.now()
      
        # æŒ‰å°æ—¶åˆ›å»ºç´¢å¼•ï¼ˆæœ€è¿‘24å°æ—¶ï¼‰
        for hour in range(24):
            index_time = today - timedelta(hours=hour)
            index_name = f"monitoring-{index_time.strftime('%Y.%m.%d.%H')}"
            self.create_hourly_index(index_name)
      
        # æŒ‰å¤©åˆ›å»ºç´¢å¼•ï¼ˆæœ€è¿‘30å¤©ï¼‰
        for day in range(30):
            index_time = today - timedelta(days=day)
            index_name = f"monitoring-daily-{index_time.strftime('%Y.%m.%d')}"
            self.create_daily_index(index_name)
  
    def search_with_index_optimization(self, query, time_range):
        """åŸºäºæ—¶é—´èŒƒå›´ä¼˜åŒ–æœç´¢ç´¢å¼•"""
        indices = self.get_relevant_indices(time_range)
      
        search_body = {
            "query": query,
            "sort": [{"@timestamp": {"order": "desc"}}],
            "size": 1000
        }
      
        return self.es_client.search(
            index=indices,
            body=search_body,
            request_timeout=30
        )
```

## 4. å¼‚å¸¸æ£€æµ‹ç®—æ³• ğŸ¤–

### å¤šæ¨¡å‹å¹¶è¡Œæ£€æµ‹æ¶æ„

```python
class AnomalyDetectionEngine:
    def __init__(self):
        self.detectors = {
            'statistical': StatisticalDetector(),
            'isolation_forest': IsolationForestDetector(),
            'lstm': LSTMDetector(),
            'rule_engine': RuleBasedDetector()
        }
        self.model_weights = {
            'statistical': 0.2,
            'isolation_forest': 0.3,
            'lstm': 0.4,
            'rule_engine': 0.1
        }
  
    async def detect_anomalies(self, data_batch):
        """å¹¶è¡Œæ‰§è¡Œå¤šç§æ£€æµ‹ç®—æ³•"""
        tasks = []
      
        for detector_name, detector in self.detectors.items():
            task = asyncio.create_task(
                self.run_detector(detector, data_batch, detector_name)
            )
            tasks.append(task)
      
        results = await asyncio.gather(*tasks)
      
        # åŠ æƒèåˆç»“æœ
        final_score = self.weighted_fusion(results)
        return self.classify_anomaly(final_score, data_batch)
  
    def weighted_fusion(self, detection_results):
        """åŠ æƒèåˆå¤šä¸ªæ£€æµ‹å™¨çš„ç»“æœ"""
        weighted_scores = []
      
        for result in detection_results:
            detector_name = result['detector']
            score = result['anomaly_score']
            weight = self.model_weights.get(detector_name, 0.25)
            weighted_scores.append(score * weight)
      
        return sum(weighted_scores)
```

### ç»Ÿè®¡é˜ˆå€¼æ£€æµ‹

```python
class StatisticalDetector:
    def __init__(self, window_size=1000, threshold_std=3):
        self.window_size = window_size
        self.threshold_std = threshold_std
        self.historical_data = deque(maxlen=window_size)
  
    def detect(self, current_value):
        """åŸºäºZ-scoreçš„å¼‚å¸¸æ£€æµ‹"""
        if len(self.historical_data) < 30:  # æœ€å°‘éœ€è¦30ä¸ªæ ·æœ¬
            self.historical_data.append(current_value)
            return {'anomaly_score': 0.0, 'is_anomaly': False}
      
        mean = np.mean(self.historical_data)
        std = np.std(self.historical_data)
      
        if std == 0:  # é¿å…é™¤é›¶é”™è¯¯
            z_score = 0
        else:
            z_score = abs(current_value - mean) / std
      
        is_anomaly = z_score > self.threshold_std
        anomaly_score = min(z_score / self.threshold_std, 1.0)
      
        self.historical_data.append(current_value)
      
        return {
            'anomaly_score': anomaly_score,
            'is_anomaly': is_anomaly,
            'z_score': z_score,
            'mean': mean,
            'std': std
        }
```

### LSTM æ—¶åºå¼‚å¸¸æ£€æµ‹

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

class LSTMDetector:
    def __init__(self, sequence_length=60, feature_dim=1):
        self.sequence_length = sequence_length
        self.feature_dim = feature_dim
        self.model = self.build_model()
        self.scaler = StandardScaler()
      
    def build_model(self):
        """æ„å»ºLSTMè‡ªç¼–ç å™¨æ¨¡å‹"""
        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(self.sequence_length, self.feature_dim)),
            Dropout(0.2),
            LSTM(32, return_sequences=True),
            Dropout(0.2),
            LSTM(16, return_sequences=False),
            Dense(16, activation='relu'),
            Dense(32, activation='relu'),
            Dense(self.sequence_length, activation='linear')
        ])
      
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        return model
  
    def train(self, normal_data):
        """ä½¿ç”¨æ­£å¸¸æ•°æ®è®­ç»ƒæ¨¡å‹"""
        # æ•°æ®é¢„å¤„ç†
        scaled_data = self.scaler.fit_transform(normal_data.reshape(-1, 1))
      
        # åˆ›å»ºåºåˆ—
        X_train = []
        for i in range(self.sequence_length, len(scaled_data)):
            X_train.append(scaled_data[i-self.sequence_length:i])
      
        X_train = np.array(X_train)
      
        # è®­ç»ƒæ¨¡å‹ï¼ˆè‡ªç¼–ç å™¨ï¼Œè¾“å…¥ç­‰äºè¾“å‡ºï¼‰
        self.model.fit(
            X_train, X_train.reshape(X_train.shape[0], -1),
            epochs=100,
            batch_size=32,
            validation_split=0.2,
            verbose=0
        )
  
    def detect(self, sequence):
        """æ£€æµ‹åºåˆ—ä¸­çš„å¼‚å¸¸"""
        if len(sequence) < self.sequence_length:
            return {'anomaly_score': 0.0, 'is_anomaly': False}
      
        # é¢„å¤„ç†
        scaled_sequence = self.scaler.transform(sequence.reshape(-1, 1))
        input_sequence = scaled_sequence[-self.sequence_length:].reshape(1, self.sequence_length, 1)
      
        # é¢„æµ‹
        reconstruction = self.model.predict(input_sequence, verbose=0)
      
        # è®¡ç®—é‡æ„è¯¯å·®
        mse = np.mean(np.square(input_sequence.flatten() - reconstruction.flatten()))
      
        # åŸºäºå†å²è¯¯å·®åˆ†å¸ƒè®¡ç®—å¼‚å¸¸åˆ†æ•°
        anomaly_score = min(mse / 0.1, 1.0)  # 0.1æ˜¯ç»éªŒé˜ˆå€¼
        is_anomaly = anomaly_score > 0.7
      
        return {
            'anomaly_score': anomaly_score,
            'is_anomaly': is_anomaly,
            'reconstruction_error': mse
        }
```

### è§„åˆ™å¼•æ“

```python
class RuleBasedDetector:
    def __init__(self):
        self.rules = []
        self.load_rules()
  
    def load_rules(self):
        """åŠ è½½æ£€æµ‹è§„åˆ™"""
        self.rules = [
            {
                'name': 'cpu_usage_spike',
                'condition': lambda data: data.get('cpu_usage', 0) > 90,
                'severity': 'high',
                'weight': 1.0
            },
            {
                'name': 'error_rate_high',
                'condition': lambda data: data.get('error_rate', 0) > 0.05,
                'severity': 'medium',
                'weight': 0.8
            },
            {
                'name': 'response_time_slow',
                'condition': lambda data: data.get('response_time', 0) > 5000,
                'severity': 'medium',
                'weight': 0.6
            },
            {
                'name': 'disk_space_low',
                'condition': lambda data: data.get('disk_usage', 0) > 85,
                'severity': 'high',
                'weight': 0.9
            }
        ]
  
    def detect(self, data):
        """åŸºäºè§„åˆ™æ£€æµ‹å¼‚å¸¸"""
        triggered_rules = []
        total_score = 0.0
      
        for rule in self.rules:
            try:
                if rule['condition'](data):
                    triggered_rules.append(rule['name'])
                    total_score += rule['weight']
            except Exception as e:
                # è§„åˆ™æ‰§è¡Œå¤±è´¥æ—¶çš„å¤„ç†
                pass
      
        # å½’ä¸€åŒ–åˆ†æ•°
        max_possible_score = sum(rule['weight'] for rule in self.rules)
        normalized_score = min(total_score / max_possible_score, 1.0) if max_possible_score > 0 else 0.0
      
        return {
            'anomaly_score': normalized_score,
            'is_anomaly': len(triggered_rules) > 0,
            'triggered_rules': triggered_rules
        }
```

## 5. èµ„æºåŠ¨æ€è°ƒåº¦ âš–ï¸

### è‡ªé€‚åº”èµ„æºç®¡ç†å™¨

```python
class ResourceManager:
    def __init__(self):
        self.k8s_client = kubernetes.client.ApiClient()
        self.metrics_client = MetricsCollector()
        self.sla_config = {
            'max_latency': 5000,  # æœ€å¤§å»¶è¿Ÿ5ç§’
            'min_throughput': 1000,  # æœ€å°ååé‡1000/ç§’
            'max_cpu_usage': 80,  # æœ€å¤§CPUä½¿ç”¨ç‡80%
            'max_memory_usage': 85  # æœ€å¤§å†…å­˜ä½¿ç”¨ç‡85%
        }
      
    async def auto_scale(self):
        """è‡ªåŠ¨æ‰©ç¼©å®¹é€»è¾‘"""
        while True:
            try:
                current_metrics = await self.collect_metrics()
                scaling_decision = self.make_scaling_decision(current_metrics)
              
                if scaling_decision['action'] != 'no_action':
                    await self.execute_scaling(scaling_decision)
              
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
              
            except Exception as e:
                logger.error(f"Auto-scaling error: {e}")
                await asyncio.sleep(60)  # é”™è¯¯æ—¶å»¶é•¿æ£€æŸ¥é—´éš”
  
    def make_scaling_decision(self, metrics):
        """åŸºäºSLAæŒ‡æ ‡åšå‡ºæ‰©ç¼©å®¹å†³ç­–"""
        current_latency = metrics.get('avg_latency', 0)
        current_throughput = metrics.get('throughput', 0)
        current_cpu = metrics.get('avg_cpu_usage', 0)
        current_memory = metrics.get('avg_memory_usage', 0)
        current_replicas = metrics.get('replica_count', 1)
      
        # æ‰©å®¹æ¡ä»¶
        should_scale_up = (
            current_latency > self.sla_config['max_latency'] or
            current_throughput < self.sla_config['min_throughput'] or
            current_cpu > self.sla_config['max_cpu_usage'] or
            current_memory > self.sla_config['max_memory_usage']
        )
      
        # ç¼©å®¹æ¡ä»¶
        should_scale_down = (
            current_latency < self.sla_config['max_latency'] * 0.5 and
            current_throughput > self.sla_config['min_throughput'] * 1.5 and
            current_cpu < self.sla_config['max_cpu_usage'] * 0.5 and
            current_memory < self.sla_config['max_memory_usage'] * 0.5 and
            current_replicas > 2  # æœ€å°‘ä¿æŒ2ä¸ªå‰¯æœ¬
        )
      
        if should_scale_up:
            new_replicas = min(current_replicas * 2, 50)  # æœ€å¤š50ä¸ªå‰¯æœ¬
            return {
                'action': 'scale_up',
                'current_replicas': current_replicas,
                'target_replicas': new_replicas,
                'reason': 'SLA violation detected'
            }
        elif should_scale_down:
            new_replicas = max(current_replicas // 2, 2)  # æœ€å°‘2ä¸ªå‰¯æœ¬
            return {
                'action': 'scale_down',
                'current_replicas': current_replicas,
                'target_replicas': new_replicas,
                'reason': 'Resource utilization low'
            }
        else:
            return {'action': 'no_action'}
```

### Kubernetes HPA é…ç½®

```yaml
# è‡ªå®šä¹‰æŒ‡æ ‡çš„HPAé…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: monitoring-hpa
  namespace: monitoring
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: monitoring-service
  minReplicas: 2
  maxReplicas: 50
  metrics:
  # CPUä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # å†…å­˜ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šé˜Ÿåˆ—é•¿åº¦
  - type: Pods
    pods:
      metric:
        name: kafka_consumer_lag
      target:
        type: AverageValue
        averageValue: "1000"
  # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šè¯·æ±‚å»¶è¿Ÿ
  - type: Pods
    pods:
      metric:
        name: request_latency_p99
      target:
        type: AverageValue
        averageValue: "5000m"  # 5ç§’
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # æ‰©å®¹ç¨³å®šçª—å£
      policies:
      - type: Percent
        value: 100  # æœ€å¤šæ‰©å®¹100%
        periodSeconds: 60
      - type: Pods
        value: 10   # æœ€å¤šä¸€æ¬¡å¢åŠ 10ä¸ªPod
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¼©å®¹ç¨³å®šçª—å£
      policies:
      - type: Percent
        value: 50   # æœ€å¤šç¼©å®¹50%
        periodSeconds: 60
```

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥

```python
class CostOptimizer:
    def __init__(self):
        self.spot_instance_manager = SpotInstanceManager()
        self.reserved_capacity = ReservedCapacityManager()
      
    def optimize_compute_costs(self, workload_forecast):
        """åŸºäºå·¥ä½œè´Ÿè½½é¢„æµ‹ä¼˜åŒ–è®¡ç®—æˆæœ¬"""
        recommendations = []
      
        # é¢„æµ‹æœªæ¥24å°æ—¶çš„è´Ÿè½½
        hourly_forecast = workload_forecast.get_hourly_prediction(24)
      
        for hour, predicted_load in enumerate(hourly_forecast):
            if predicted_load < 0.3:  # ä½è´Ÿè½½æ—¶æ®µ
                recommendations.append({
                    'time': hour,
                    'action': 'use_spot_instances',
                    'target_spot_ratio': 0.8,  # 80%ä½¿ç”¨Spotå®ä¾‹
                    'expected_savings': 0.6
                })
            elif predicted_load > 0.8:  # é«˜è´Ÿè½½æ—¶æ®µ
                recommendations.append({
                    'time': hour,
                    'action': 'scale_to_on_demand',
                    'target_spot_ratio': 0.3,  # 30%ä½¿ç”¨Spotå®ä¾‹
                    'expected_savings': 0.2
                })
      
        return recommendations
  
    def implement_cost_optimization(self, recommendations):
        """å®æ–½æˆæœ¬ä¼˜åŒ–å»ºè®®"""
        for rec in recommendations:
            if rec['action'] == 'use_spot_instances':
                self.spot_instance_manager.increase_spot_ratio(
                    target_ratio=rec['target_spot_ratio']
                )
            elif rec['action'] == 'scale_to_on_demand':
                self.spot_instance_manager.decrease_spot_ratio(
                    target_ratio=rec['target_spot_ratio']
                )
```

## 6. æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ ğŸš€

### ç¼“å­˜ç­–ç•¥

```python
class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜
        self.l2_cache = redis.Redis(host='redis-cluster')  # Redisç¼“å­˜
        self.l3_cache = memcached.Client(['memcached1:11211'])  # Memcached
      
    async def get(self, key):
        """å¤šçº§ç¼“å­˜è¯»å–"""
        # L1: å†…å­˜ç¼“å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]
      
        # L2: Redisç¼“å­˜
        value = await self.l2_cache.get(key)
        if value:
            self.l1_cache[key] = value  # å†™å…¥L1ç¼“å­˜
            return value
      
        # L3: Memcached
        value = self.l3_cache.get(key)
        if value:
            await self.l2_cache.setex(key, 3600, value)  # å†™å…¥L2ç¼“å­˜
            self.l1_cache[key] = value  # å†™å…¥L1ç¼“å­˜
            return value
      
        return None
  
    async def set(self, key, value, ttl=3600):
        """å¤šçº§ç¼“å­˜å†™å…¥"""
        # å†™å…¥æ‰€æœ‰çº§åˆ«çš„ç¼“å­˜
        self.l1_cache[key] = value
        await self.l2_cache.setex(key, ttl, value)
        self.l3_cache.set(key, value, time=ttl)
```

### æ‰¹å¤„ç†ä¼˜åŒ–

```python
class BatchProcessor:
    def __init__(self, batch_size=1000, max_wait_time=5):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_batch = []
        self.last_flush_time = time.time()
      
    async def add_item(self, item):
        """æ·»åŠ é¡¹ç›®åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        self.pending_batch.append(item)
      
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ‰¹æ¬¡
        should_flush = (
            len(self.pending_batch) >= self.batch_size or
            time.time() - self.last_flush_time >= self.max_wait_time
        )
      
        if should_flush:
            await self.flush_batch()
  
    async def flush_batch(self):
        """åˆ·æ–°å½“å‰æ‰¹æ¬¡"""
        if not self.pending_batch:
            return
      
        batch_to_process = self.pending_batch.copy()
        self.pending_batch.clear()
        self.last_flush_time = time.time()
      
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        await self.process_batch(batch_to_process)
  
    async def process_batch(self, batch):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        tasks = []
        chunk_size = 100
      
        for i in range(0, len(batch), chunk_size):
            chunk = batch[i:i + chunk_size]
            task = asyncio.create_task(self.process_chunk(chunk))
            tasks.append(task)
      
        await asyncio.gather(*tasks)
```

## 7. ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ ğŸ“Š

### å¯è§†åŒ–çœ‹æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "Selfdrived ç›‘æ§æ€»è§ˆ",
    "panels": [
      {
        "title": "å®æ—¶æ•°æ®æµé‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(data_ingestion_total[5m])",
            "legendFormat": "æ•°æ®æ‘„å–é€Ÿç‡"
          }
        ]
      },
      {
        "title": "å¼‚å¸¸æ£€æµ‹ç»“æœ",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(anomaly_detection_alerts_total)",
            "legendFormat": "æ€»å¼‚å¸¸æ•°"
          }
        ]
      },
      {
        "title": "ç³»ç»Ÿèµ„æºä½¿ç”¨",
        "type": "heatmap",
        "targets": [
          {
            "expr": "avg(cpu_usage) by (instance)",
            "legendFormat": "CPUä½¿ç”¨ç‡"
          },
          {
            "expr": "avg(memory_usage) by (instance)",
            "legendFormat": "å†…å­˜ä½¿ç”¨ç‡"
          }
        ]
      },
      {
        "title": "SLAæŒ‡æ ‡",
        "type": "gauge",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, request_duration_seconds_bucket)",
            "legendFormat": "P99å»¶è¿Ÿ"
          }
        ]
      }
    ]
  }
}
```

### æ™ºèƒ½å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™
groups:
- name: selfdrived.rules
  rules:
  # æ•°æ®æµé‡å¼‚å¸¸
  - alert: DataIngestionRateDrop
    expr: rate(data_ingestion_total[5m]) < 100
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "æ•°æ®æ‘„å–é€Ÿç‡å¼‚å¸¸ä¸‹é™"
      description: "æ•°æ®æ‘„å–é€Ÿç‡åœ¨è¿‡å»5åˆ†é’Ÿå†…ä½äº100/ç§’ï¼ŒæŒç»­2åˆ†é’Ÿ"
  
  # å¼‚å¸¸æ£€æµ‹æ¨¡å‹å¤±æ•ˆ
  - alert: AnomalyDetectionModelDown
    expr: up{job="anomaly-detection"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "å¼‚å¸¸æ£€æµ‹æ¨¡å‹æœåŠ¡ä¸å¯ç”¨"
      description: "å¼‚å¸¸æ£€æµ‹æœåŠ¡å·²åœæ­¢å“åº”è¶…è¿‡1åˆ†é’Ÿ"
  
  # SLAè¿è§„
  - alert: SLAViolation
    expr: histogram_quantile(0.99, request_duration_seconds_bucket) > 5
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "SLAè¿è§„ï¼šå“åº”æ—¶é—´è¿‡é«˜"
      description: "P99å“åº”æ—¶é—´è¶…è¿‡5ç§’ï¼ŒæŒç»­5åˆ†é’Ÿ"
  
  # èµ„æºä½¿ç”¨å¼‚å¸¸
  - alert: HighResourceUsage
    expr: avg(cpu_usage) > 90 or avg(memory_usage) > 95
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡è¿‡é«˜"
      description: "CPUæˆ–å†…å­˜ä½¿ç”¨ç‡æŒç»­3åˆ†é’Ÿè¶…è¿‡é˜ˆå€¼"
```

## ğŸ¯ æ€»ç»“

Selfdrived å¤§è§„æ¨¡æ•°æ®ç›‘æ§æ¶æ„é€šè¿‡ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯å®ç°äº†å¯¹æµ·é‡ä¿¡æ¯çš„æŒç»­ã€æ™ºèƒ½ç›‘æ§ï¼š

### å…³é”®æŠ€æœ¯ä¼˜åŠ¿

1. **é«˜å¯æ‰©å±•æ€§**ï¼šåˆ†å¸ƒå¼æ¶æ„æ”¯æŒæ¨ªå‘æ‰©å±•ï¼Œèƒ½å¤Ÿå¤„ç†PBçº§æ•°æ®
2. **å®æ—¶æ€§**ï¼šæµå¤„ç†æŠ€æœ¯ç¡®ä¿æ¯«ç§’çº§çš„æ•°æ®å¤„ç†å»¶è¿Ÿ
3. **æ™ºèƒ½åŒ–**ï¼šå¤šæ¨¡å‹èåˆçš„å¼‚å¸¸æ£€æµ‹ï¼Œæä¾›å‡†ç¡®çš„å¼‚å¸¸è¯†åˆ«
4. **è‡ªé€‚åº”**ï¼šåŠ¨æ€èµ„æºè°ƒåº¦å’Œæˆæœ¬ä¼˜åŒ–ï¼Œå®ç°è‡ªåŠ¨åŒ–è¿ç»´
5. **é«˜å¯ç”¨æ€§**ï¼šå¤šçº§ç¼“å­˜å’Œæ•…éšœæ¢å¤æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§

### å®æ–½å»ºè®®

1. **åˆ†é˜¶æ®µå®æ–½**ï¼šä»æ ¸å¿ƒç»„ä»¶å¼€å§‹ï¼Œé€æ­¥æ‰©å±•åŠŸèƒ½æ¨¡å—
2. **ç›‘æ§ä¼˜å…ˆ**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼Œç¡®ä¿ç³»ç»Ÿå¯è§‚æµ‹æ€§
3. **æ€§èƒ½è°ƒä¼˜**ï¼šæ ¹æ®å®é™…è´Ÿè½½æƒ…å†µè°ƒæ•´é…ç½®å‚æ•°
4. **æˆæœ¬æ§åˆ¶**ï¼šåˆç†ä½¿ç”¨äº‘èµ„æºï¼Œå®æ–½æˆæœ¬ä¼˜åŒ–ç­–ç•¥

è¿™å¥—æ¶æ„ä¸ä»…èƒ½å¤Ÿæ»¡è¶³å½“å‰çš„ç›‘æ§éœ€æ±‚ï¼Œè¿˜å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿé€‚åº”æœªæ¥ä¸šåŠ¡å¢é•¿å’ŒæŠ€æœ¯æ¼”è¿›çš„éœ€è¦ã€‚é€šè¿‡æŒç»­ä¼˜åŒ–å’Œè°ƒæ•´ï¼Œå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªé«˜æ•ˆã€å¯é ã€æ™ºèƒ½çš„å¤§è§„æ¨¡æ•°æ®ç›‘æ§ç³»ç»Ÿã€‚

---

**æ ‡ç­¾**: `å¤§æ•°æ®` `å®æ—¶ç›‘æ§` `å¼‚å¸¸æ£€æµ‹` `åˆ†å¸ƒå¼ç³»ç»Ÿ` `å¾®æœåŠ¡æ¶æ„` `æœºå™¨å­¦ä¹ ` `è‡ªåŠ¨åŒ–è¿ç»´`

**ä½œè€…**: Lvy
**æ›´æ–°æ—¶é—´**: 2025å¹´8æœˆ
**ç‰ˆæœ¬**: v1.0
